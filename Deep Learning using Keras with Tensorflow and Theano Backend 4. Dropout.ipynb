{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 16:\n",
    "    Reduce Overfitting with Dropout Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Import modules\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.constraints import maxnorm\n",
    "from keras.optimizers import SGD\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "# fix random seed for reproducibility\n",
    "seed = 7 \n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " AMMI_Models.py\r\n",
      " baseline_model_plot.png\r\n",
      "'Deep Learning using Keras with Tensorflow and Theano Backend 4. Dropout.ipynb'\r\n",
      "'Deep Learning with Keras using Tensorflow and Theano backend 2. Models..ipynb'\r\n",
      "'Deep learning with Keras, using Tensorflow and Theano backend 3. Model checkpointing.ipynb'\r\n",
      "'Deep Learning with Keras using Tensorflow and Theano backend, Python 1. Keras Structure..ipynb'\r\n",
      " iris.data\r\n",
      "'Linear Algebra and Basics Mathematics..ipynb'\r\n",
      " model_plot.png\r\n",
      " Models\r\n",
      " Models_Boston_Data\r\n",
      " pima-indians-diabetes.data\r\n",
      " __pycache__\r\n",
      " sonar.all-data\r\n",
      " Weights\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Sonar dataset\n",
    "data = pd.read_csv('sonar.all-data', header = None)\n",
    "dataset = data.values\n",
    "# split data into X and Y\n",
    "X = dataset[:, :60].astype(float)\n",
    "Y = dataset[:,60]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode class values as integers\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(Y)\n",
    "encoded_Y = encoder.transform(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create baseline model\n",
    "def baseline():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(60, input_dim=60, kernel_initializer='uniform', activation='relu'))\n",
    "    model.add(Dense(30, kernel_initializer='uniform', activation = 'relu'))\n",
    "    model.add(Dense(1, kernel_initializer='uniform', activation = 'sigmoid'))\n",
    "    # Compile the model\n",
    "    sgd = SGD(learning_rate=0.01, momentum=0.8, decay =0.0, nesterov = False)\n",
    "    model.compile(loss = 'binary_crossentropy', optimizer = sgd, metrics = ['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline: 85.56% (8.65%)\n"
     ]
    }
   ],
   "source": [
    "estimator = []\n",
    "estimator.append(('standardize', StandardScaler()))\n",
    "estimator.append(('mlp', KerasClassifier(build_fn=baseline, epochs=300, batch_size=16, verbose=0)))\n",
    "pipeline = Pipeline(estimator)\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "results = cross_val_score(pipeline, X, encoded_Y, cv = kfold)\n",
    "print('Baseline: %.2f%% (%.2f%%)' % (results.mean()*100, results.std()*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using Dropout on the visible layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model\n",
    "def baseline2_dp():\n",
    "    model = Sequential()\n",
    "    model.add(Dropout(0.2, input_shape=(60, )))\n",
    "    model.add(Dense(60, kernel_initializer='uniform', activation='relu', W_constraint=maxnorm(3)))\n",
    "    model.add(Dense(30, kernel_initializer='uniform', activation = 'relu', W_constraint = maxnorm(3)))\n",
    "    model.add(Dense(1, kernel_initializer='uniform', activation = 'sigmoid'))\n",
    "    # Compile the model\n",
    "    sgd = SGD(learning_rate=0.1, momentum=0.9, decay =0.0, nesterov = False)\n",
    "    model.compile(loss = 'binary_crossentropy', optimizer = sgd, metrics = ['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline: 85.09% (5.87%)\n"
     ]
    }
   ],
   "source": [
    "estimator = []\n",
    "estimator.append(('standardize', StandardScaler()))\n",
    "estimator.append(('mlp', KerasClassifier(build_fn=baseline_dp, epochs=300, batch_size=16, verbose=0)))\n",
    "pipeline = Pipeline(estimator)\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "results = cross_val_score(pipeline, X, encoded_Y, cv = kfold)\n",
    "print('Baseline: %.2f%% (%.2f%%)' % (results.mean()*100, results.std()*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.3.1'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import keras\n",
    "keras.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dropout in the hidden layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model\n",
    "def baseline3_dp():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(60, input_dim = 60, kernel_initializer='uniform', activation='relu', kernel_constraint=maxnorm(3)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(30, kernel_initializer='uniform', activation = 'relu', kernel_constraint = maxnorm(3)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(1, kernel_initializer='uniform', activation = 'sigmoid'))\n",
    "    # Compile the model\n",
    "    sgd = SGD(learning_rate=0.1, momentum=0.9, decay =0.0, nesterov = False)\n",
    "    model.compile(loss = 'binary_crossentropy', optimizer = sgd, metrics = ['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline: 84.59% (6.01%)\n"
     ]
    }
   ],
   "source": [
    "estimator = []\n",
    "estimator.append(('standardize', StandardScaler()))\n",
    "estimator.append(('mlp', KerasClassifier(build_fn=baseline3_dp, epochs=300, batch_size=16, verbose=0)))\n",
    "pipeline = Pipeline(estimator)\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "results = cross_val_score(pipeline, X, encoded_Y, cv = kfold)\n",
    "print('Baseline: %.2f%% (%.2f%%)' % (results.mean()*100, results.std()*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes on using dropouts:\n",
    "    1. Generally use a small dropout value of 20%-50% of neurons with 20% providing a good starting point. A probability too low has minimal effect and a value too high results in under-learning by the network.\n",
    "    2. Use a larger network. You are likely to get better performance when dropout is used on a larger network, giving the model more of an opportunity to learn independent representations.\n",
    "    3. Use dropout on input (visible) as well as hidden layers. Application of dropout at each layer of the network has shown good results.\n",
    "    4. Use a large learning rate with decay and a large momentum. Increase your learning rate by a factor of 10 to 100 and use a high momentum value of 0.9 or 0.99.\n",
    "    5. Constrain the size of network weights. A large learning rate can result in very large network weights. Imposing a constraint on the size of network weights such as max-norm regularization with a size of 4 or 5 has been shown to improve results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 17:\n",
    "    Lift performance with learning rate schedules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-03-01 16:08:02--  http://archive.ics.uci.edu/ml/machine-learning-databases/ionosphere/ionosphere.data\n",
      "Resolving archive.ics.uci.edu (archive.ics.uci.edu)... 128.195.10.252\n",
      "Connecting to archive.ics.uci.edu (archive.ics.uci.edu)|128.195.10.252|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 76467 (75K) [application/x-httpd-php]\n",
      "Saving to: ‘ionosphere.data’\n",
      "\n",
      "ionosphere.data     100%[===================>]  74.67K  32.9KB/s    in 2.3s    \n",
      "\n",
      "2020-03-01 16:08:06 (32.9 KB/s) - ‘ionosphere.data’ saved [76467/76467]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Downlod ionosphere data\n",
    "!wget http://archive.ics.uci.edu/ml/machine-learning-databases/ionosphere/ionosphere.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Understanding Learning rate schedulers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 50)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set decay constant, lr and starting epoch \n",
    "decay = 0.001\n",
    "lr = 0.01\n",
    "epoch = 0\n",
    "# collect lr and epoch values\n",
    "lr_list = []\n",
    "epoch_list = []\n",
    "# setup while loop for learning rate update\n",
    "while epoch < 50:\n",
    "    lr = lr * (1/(1+decay*epoch))\n",
    "    epoch_list.append(epoch)\n",
    "    lr_list.append(lr)\n",
    "    epoch += 1    \n",
    "    # print(lr)\n",
    "# Verify the length of the lists.\n",
    "len(epoch_list), len(lr_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import module to plot\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAD8CAYAAAB3u9PLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAIABJREFUeJzt3Xl8VfWd//HXJysk7CEsEpYAQQ0gCGFHrMUqWAQX1FAXVBSLuNV2Wp352WmdtlM7ndGKoKJYcRkBQTF1t1orOwQE2UTCHkEW2dcQ+Pz+uIdpGgNcIOHe3LyfjweP3PM933Pv59vGvO8553vOMXdHREQkLtIFiIhIdFAgiIgIoEAQEZGAAkFERAAFgoiIBBQIIiICKBBERCSgQBAREUCBICIigYRIF3Aq6tev7y1atIh0GSIilcb8+fO3uXt6OH0rVSC0aNGC/Pz8SJchIlJpmNm6cPvqkJGIiAAKBBERCSgQREQEUCCIiEhAgSAiIkCYgWBm/cxshZkVmNlDZaxPNrOJwfo5ZtYiaE8zs7+Z2V4ze6rUNp3NbHGwzZNmZuUxIBEROT0nDQQziwdGA/2BbGCImWWX6jYM2OHurYHHgceC9oPAI8DPynjrp4HhQFbwr9/pDEBERMpHONchdAUK3H01gJlNAAYBy0r0GQT8Kng9GXjKzMzd9wHTzax1yTc0s8ZALXefFSy/BFwFvHcGYzmuJz9eSXycUT0xntTkeFKSEkhJCv2sVT2B9JrJ1EtJIiFeR9BEpOoKJxCaABtKLBcC3Y7Xx92LzWwXkAZsO8F7FpZ6zyZldTSz4YT2JGjWrFkY5X7XM39fxf6iIyfsYwb1UpKoXyOZ9JrJNKiVTLN6KTSrl0LztBSa1Uulfo0kdGRLRGJVOIFQ1l9AP40+p9Xf3ccCYwFycnJO9J7HtezRfhQVH2V/UTH7i46wv6iYfYeOsK+omF37D7Nt7yG27i1i295DbNtziG17DzF71V7e/PxrvMQnpiTF06xeCuc1qsl5jWtxXqOanN+4Fg1qJisoRKTSCycQCoGmJZYzgI3H6VNoZglAbWD7Sd4z4yTvWa6SEuJISkiiTkr42xwqPkLhjgOs/3Y/677dx7rt+1m7bR9z12xn6sJ/lFs3JZHzGtWiQ9M6dGpWh07N61K/RnIFjEJEpOKEEwjzgCwzywS+BnKBH5XqkwcMBWYBg4FP3P243+bdfZOZ7TGz7sAc4BZg1GnUX6GSE+JplV6DVuk1vrNu1/7DfPnNbr78Zg9ffrObZRt3M276ap45Ehp287QUOjWrS6fmdemWWY+sBjW0FyEiUe2kgRCcE7gH+ACIB15w96Vm9iiQ7+55wDjgZTMrILRnkHtsezNbC9QCkszsKuAyd18GjABeBKoTOplcISeUK0rtlES6tUyjW8u0/2s7ePgIS77exfx1O1iwfgfTVm7jzc+/BqBBzWR6t65Pr9b16Z1Vn4a1qkWqdBGRMtkJvshHnZycHK9Mdzt1dzZsP8Cs1duYXvAtMwq2sX1fEQBZDWrQp006P8huSE7zuprhJCIVwszmu3tOWH0VCGfP0aPO8m92M6NgG9NWbmPOmu0UFR+lTkoi3z+vAZdlN6RPm3RSkirVXclFJIopECqJfYeK+eyrrXy4bDOffLmFXQcOk5QQR5+s+lzZ4Rx+kN1Q4SAiZ+RUAkF/bSIoNTmB/u0b0799Yw4fOcq8tdv5aNlm3l/yDX9dvoXqifH8ILshgzqew0VZ6SQl6LCSiFQc7SFEoaNHnXlrt/PWoo28u3gTO/cfpk5KIle0b8x1nTPo2LSOZiyJSFh0yCiGFBUfZXrBVt5auJEPl27mwOEjnNuwJjd0acrVFzahbmpSpEsUkSimQIhRew4e5i+LNjFx3noWFe4iKT6Oy9s1IrdLU3q0TCMuTnsNIvLPFAhVwLKNu5mUv4E3FhSy+2AxLdNTubVnC67plEGNZJ0aEpEQBUIVcvDwEd5bsokXZ6xlUeEuaiYncF1OU4b2bE7ztNRIlyciEaZAqKI+X7+DF2eu5Z0vNnHEnb7nNeD23pn0aJmmk9AiVZQCoYrbvPsgr85Zz//OWce2vUV0yKjNiO+14rLsRjrPIFLFKBAECB1OmrKgkLGfrWbdt/tpmZ7KXX1actWFTUhOiI90eSJyFigQ5J8cOeq8t2QTT3+6iqUbd9OwVjJ3XtSSG7s1p3qSgkEklikQpEzuzrSV2xjzaQGzV2+nfo1kRnyvFTd2a0a1RAWDSCxSIMhJzV2znSf++hUzV31Lg5qhYBjSVcEgEmsUCBK22au/5Ym/fsXs1dtpWCuZkZe0JrdLM903SSRGKBDklM1ctY0nPlrJ3LXbaVYvhZ9e1oYrLzhHs5JEKrlTCQR9DRQAeraqz8S7ujP+9q6kJidw/4SFXPnUdKat3Brp0kTkLFEgyP8xMy5uk8479/bmiRs6suvAYW4eN5ebx81hyde7Il2eiFQwBYJ8R1yccdWFTfj4pxfzyIBslny9iwGjpvPgxIV8s+tgpMsTkQoSViCYWT8zW2FmBWb2UBnrk81sYrB+jpm1KLHu4aB9hZldXqL9fjNbYmZLzeyB8hiMlK/khHiG9c7k7z+/hBHfa8XbX2zikj9+yqiPV3Lw8JFIlyci5eykgWBm8cBooD+QDQwxs+xS3YYBO9y9NfA48FiwbTaQC7QF+gFjzCzezNoBdwJdgQ7AADPLKp8hSXmrVS2RX/Q7j78+eDEXt0nnvz/6ir7//Xf+smgjlWlSgoicWDh7CF2BAndf7e5FwARgUKk+g4DxwevJQF8L3U1tEDDB3Q+5+xqgIHi/84HZ7r7f3YuBvwNXn/lwpCI1S0vhmZs789qd3alVPZF7X/uc656ZpfMLIjEinEBoAmwosVwYtJXZJ/gDvwtIO8G2S4A+ZpZmZinAFUDTsj7czIabWb6Z5W/dqhkv0aBHqzTevrc3/3lNe9Zs28fAp6bz728tYdeBw5EuTUTOQDiBUNZE9NLHCY7Xp8x2d19O6LDSR8D7wCKguKwPd/ex7p7j7jnp6elhlCtnQ3ycMaRrMz752fe4uXtzXp69jr7//SlT5hfqMJJIJRVOIBTyz9/eM4CNx+tjZglAbWD7ibZ193Hu3snd+wR9V57OACSyaldP5NeD2pF3T28y6qbw09cXccOzs1nxzZ5IlyYipyicQJgHZJlZppklETpJnFeqTx4wNHg9GPjEQ18T84DcYBZSJpAFzAUwswbBz2bANcBrZzoYiZx2TWrzxoie/P6a9qzcsocrnpzG795dzv6iMnf8RCQKnfThu+5ebGb3AB8A8cAL7r7UzB4F8t09DxgHvGxmBYS+7ecG2y41s0nAMkKHhEa6+7H5ilPMLA04HLTvKO/BydkVF2fkdm3G5W0b8YcPvmTsZ6t5d/Emfnt1ey5uo8N9ItFO9zKSCjNn9bc8/OZiVm/dx1Udz+GRAdmk1UiOdFkiVYruZSRRoVvLNN67/yLu65vFO4s30fd//s5knXQWiVoKBKlQyQnxPPiDNrx730W0Sq/Bz15fxM3j5rJh+/5IlyYipSgQ5KzIaliT1+/qwX9c1Y7P1+/g8ic+4+XZ6zh6VHsLItFCgSBnTVyccXP35nzwkz50bl6XR6Yu4cbn52hvQSRKKBDkrMuom8JLt3fl99e0Z/HXu7j8ic94adZa7S2IRJgCQSLCLDRF9cOf9KFLi3r88q2lDHlutvYWRCJIgSARdU6d6rx4Wxf+cO0FLNu4m35PfMbEees1E0kkAhQIEnFmxvVdmvLeAxdxQUYdfjFlMXe+lM/WPYciXZpIlaJAkKiRUTeFV+/oxiMDsvls5TYuf+Iz3l/yTaTLEqkyFAgSVeLijGG9M3nn3t6cU6caP35lPj+dtIjdB3VrbZGKpkCQqJTVsCZvjOjFfd9vzdSFX3PFn6Yxf932SJclEtMUCBK1khLiePCyc5l0Vw/M4PpnZ/PEX7+i+MjRSJcmEpMUCBL1Ojevy7v3XcTADufwxF9XcsNYTU8VqQgKBKkUalZL5PEbOvKn3I589c0ervjTNN5a+HWkyxKJKQoEqVQGdWzCu/dfRJtGNbl/wkIenLiQfYf0EB6R8qBAkEqnab0UJg7vzv19s5i68GuuHDWdpRt3RboskUpPgSCVUkJ8HD/5QRtevaM7+4qKuXrMTF6atVZXOIucAQWCVGo9WqXx7n0X0atVGr98ayk/fmU+u/brmgWR06FAkEovrUYy44Z24d+uOJ+Pl2/hiienMX+dHtEtcqrCCgQz62dmK8yswMweKmN9splNDNbPMbMWJdY9HLSvMLPLS7T/xMyWmtkSM3vNzKqVx4CkaoqLM+7s05LJI3oSFwc3PDuL56et1iEkkVNw0kAws3hgNNAfyAaGmFl2qW7DgB3u3hp4HHgs2DYbyAXaAv2AMWYWb2ZNgPuAHHdvB8QH/UTOSMemdXj73ovoe34DfvPOcu56eT67DugQkkg4wtlD6AoUuPtqdy8CJgCDSvUZBIwPXk8G+pqZBe0T3P2Qu68BCoL3A0gAqptZApACbDyzoYiE1K6eyDM3deaRAdl88uUWBoyaxuJCzUISOZlwAqEJsKHEcmHQVmYfdy8GdgFpx9vW3b8G/gisBzYBu9z9w9MZgEhZzEI3yZv04x4cOeJc+/RMXp69ToeQRE4gnECwMtpK/1d1vD5ltptZXUJ7D5nAOUCqmd1U5oebDTezfDPL37p1axjlivxDp2Z1eee+i+jZOo1Hpi7h/gm6kE3keMIJhEKgaYnlDL57eOf/+gSHgGoD20+w7aXAGnff6u6HgTeAnmV9uLuPdfccd89JT08Po1yRf1Y3NYkXhnbhXy4/l7e/2MhVo2dQsGVvpMsSiTrhBMI8IMvMMs0sidDJ37xSffKAocHrwcAnHto3zwNyg1lImUAWMJfQoaLuZpYSnGvoCyw/8+GIlC0uzhh5SWteHtaN7fuKGPTUdN5bvCnSZYlElZMGQnBO4B7gA0J/tCe5+1Ize9TMBgbdxgFpZlYAPAg8FGy7FJgELAPeB0a6+xF3n0Po5PMCYHFQx9hyHZlIGXq1rs/b9/Umq2FNRry6gN+9u1y30xYJWGU6yZaTk+P5+fmRLkNiQFHxUX7zzjJemrWObpn1GPWjC2lQU5fCSOwxs/nunhNOX12pLFVSUkIcjw5qxxM3dGRR4U4GPDldT2STKk+BIFXaVRc2YerIXlRPiid37Gxe0dRUqcIUCFLlndeoFnkje9O7dX3+39Ql/GLKFxw8fCTSZYmcdQoEEaB2SiLjhnbhvu+3ZlJ+ITc8O4uNOw9EuiyRs0qBIBKIizMevOxcnr25M6u27uPKUdOZterbSJclctYoEERKubxtI6aO7EXtlERuGjeHP89Yo/MKUiUoEETK0LpBDd4a2YtLzm3Ar/+yjH+ZrPMKEvsUCCLHUbNaImNv7sx9fbOYPL+Q3LGz2bz7YKTLEqkwCgSRE4iLMx78QRueuakTX23ew5WjprNgvZ7GJrFJgSAShn7tGvPG3T1JTowj99nZTMrfcPKNRCoZBYJImI5dr9Alsy4/n/wFv8pbqvsgSUxRIIicgrqpSYy/rSu398rkxZlrufXP89i5vyjSZYmUCwWCyClKiI/jl1dm84drL2DOmm+D5yvsiXRZImdMgSBymq7v0pTX7uzO3kPFXD16Jn9bsSXSJYmcEQWCyBnIaVGPt+7pTdN6KQx7cR7PfbZaF7FJpaVAEDlDTepUZ/KIHvRv15jfvrucn73+BYeKdRGbVD4KBJFykJKUwFM/upCfXNqGKQsKufG5OWzbeyjSZYmcEgWCSDkxM+6/NIvRP+rEko27GPTUDJZv2h3pskTCpkAQKWc/vKAxr9/Vk+KjRxn89Ez+umxzpEsSCUtYgWBm/cxshZkVmNlDZaxPNrOJwfo5ZtaixLqHg/YVZnZ50HaumS0s8W+3mT1QXoMSibT2GbXJu6c3rRrU4M6X83nm76t0slmi3kkDwczigdFAfyAbGGJm2aW6DQN2uHtr4HHgsWDbbCAXaAv0A8aYWby7r3D3ju7eEegM7AfeLKcxiUSFhrWqMXF4D65o35jfv/elTjZL1AtnD6ErUODuq929CJgADCrVZxAwPng9GehrZha0T3D3Q+6+BigI3q+kvsAqd193uoMQiVbVk+J5asiFPHBpFlMWFHLz83PZvk9XNkt0CicQmgAl7+RVGLSV2cfdi4FdQFqY2+YCr4VfskjlYmY8cGkbnhxyIQsLd+rKZola4QSCldFW+mDo8fqccFszSwIGAq8f98PNhptZvpnlb926NYxyRaLTwA7nMGF4d/YXFXP1mJlMW6nfZ4ku4QRCIdC0xHIGsPF4fcwsAagNbA9j2/7AAnc/7jQMdx/r7jnunpOenh5GuSLRq1Ozukwd2Ysmdapz65/n8fJsHSmV6BFOIMwDsswsM/hGnwvkleqTBwwNXg8GPvHQlIo8IDeYhZQJZAFzS2w3BB0ukiomo24Kk0f05OI26TwydYluoy1R46SBEJwTuAf4AFgOTHL3pWb2qJkNDLqNA9LMrAB4EHgo2HYpMAlYBrwPjHT3IwBmlgL8AHijfIckEv1qJCfw3C05DOsduo328Jfns/dQcaTLkirOKtPc6JycHM/Pz490GSLl6tU56/jlW0vJalCDcbd2oUmd6pEuSWKImc1395xw+upKZZEIu7Fbc/58axe+3nGAq0bPYNGGnZEuSaooBYJIFOjTJp0pd/ckOSGOG8bO4v0lmyJdklRBCgSRKNGmYU3evLsX5zeuxY9fWaDbXchZp0AQiSLpNZN57c7uDLggdLuLh6Ys5rBmIMlZkhDpAkTkn1VLjOfJ3AvJrJ/KqE8KKNy5nzE3dqZ29cRIlyYxTnsIIlEoLs746WXn8sfrOjB3zXaufXomG7bvj3RZEuMUCCJRbHDnDF66vRtb9xziqtEzWLB+R6RLkhimQBCJcj1apfHG3T2pUS2BIWNn884XmoEkFUOBIFIJtEqvwZt396J9k9qM/N8FjP5bgWYgSblTIIhUEvVSk3jljm4M7HAO//XBCs1AknKnWUYilUi1xHj+lNuRFmkpPKkZSFLOtIcgUsmYGQ9edi7/NfgC5qzezmDNQJJyokAQqaSuy2nKS7d3ZfPug1w9ZiYLdQ8kOUMKBJFKrGfr+rxxd0+qJ8WRq3sgyRlSIIhUcq0b/OMeSCNeXcBzn63WDCQ5LQoEkRhQv0boHkj92zXit+8u55G3lugpbHLKFAgiMaJaYjxPDenEXRe35JXZ67njpXw9hU1OiQJBJIbExRkP9z+f313dnmkrt3HdM7PYtOtApMuSSkKBIBKDftStGS/c2oUN2/dz1egZLN24K9IlSSUQViCYWT8zW2FmBWb2UBnrk81sYrB+jpm1KLHu4aB9hZldXqK9jplNNrMvzWy5mfUojwGJSMjFbdJ5/cc9iDPjumdm8bcvt0S6JIlyJw0EM4sHRgP9gWxgiJlll+o2DNjh7q2Bx4HHgm2zgVygLdAPGBO8H8CfgPfd/TygA7D8zIcjIiWd37gWU0f2IrN+KsPGz+PlWWsjXZJEsXD2ELoCBe6+2t2LgAnAoFJ9BgHjg9eTgb5mZkH7BHc/5O5rgAKgq5nVAvoA4wDcvcjddVWNSAVoWKsak+7qwSXnNuCRt5bym7eXceSopqXKd4UTCE2ADSWWC4O2Mvu4ezGwC0g7wbYtga3An83sczN73sxST2sEInJSqckJjL0lh6E9mvP89DXc/ep8DhQdiXRZEmXCCQQro63014vj9TleewLQCXja3S8E9gHfOTcBYGbDzSzfzPK3bt0aRrkiUpb4OOPXg9rxywHZfLhsM7ljZ7Flz8FIlyVRJJxAKASalljOADYer4+ZJQC1ge0n2LYQKHT3OUH7ZEIB8R3uPtbdc9w9Jz09PYxyReREbu+dybM3dearzXu5evRMvtq8J9IlSZQIJxDmAVlmlmlmSYROEueV6pMHDA1eDwY+8dC183lAbjALKRPIAua6+zfABjM7N9imL7DsDMciImG6rG0jJt7VnaIjR7n26ZnMKNgW6ZIkCpw0EIJzAvcAHxCaCTTJ3Zea2aNmNjDoNg5IM7MC4EGCwz/uvhSYROiP/fvASHc/duDyXuBVM/sC6Aj8rvyGJSInc0FGHd68uyfn1K7O0BfmMmnehpNvJDHNKtNNsHJycjw/Pz/SZYjElN0HDzPy1QVMW7mNkZe04qc/OJe4uLJO/0llZGbz3T0nnL66UlmkiqtVLZEXbu3CkK5NGf23Vdw34XMOHtYMpKpIj9AUERLj4/jd1e1pnpbK79/7kk27DvLcLTnUS02KdGlyFmkPQUSA0KM5f3xxK8bc2IklX+/i6jEzWLV1b6TLkrNIgSAi/+SK9o15bXh39h4s5poxM5m9+ttIlyRniQJBRL6jU7O6TB3Zi/o1krh53BymzC+MdElyFigQRKRMTeul8MaIXnRpUY+fvr6I//noKz2aM8YpEETkuGqnJPLibV25rnMGT368kgcmLtQMpBimWUYickJJCXH8YfAFtKifyn99sIKvdxxgrGYgxSTtIYjISZkZIy9pzaghF/KFZiDFLAWCiITtyg7n8NqdoRlIV4+ewcxVugdSLFEgiMgp6dw8NAOpQa1q3DJuLpPydQ+kWKFAEJFT1rReClNG9KRHqzR+PvkL/vD+lxzVU9gqPQWCiJyW2tWP3QOpGWM+XcW9r+keSJWdZhmJyGkL3QOpHS3rp/K795ZTuPMAz93SmQY1q0W6NDkN2kMQkTNiZtzZp2XoKWzf7OGqp2awfNPuSJclp0GBICLl4rK2jXj9xz046jD46Zl88uXmSJckp0iBICLlpl2T2kwd2YvM9FTuGJ/PC9PX6HYXlYgCQUTKVaPa1Zh0Vw8uPb8hj769jEfeWsLhI0cjXZaEQYEgIuUuJSmBZ27qzF0Xt+SV2eu57c/z2HXgcKTLkpNQIIhIhYiLMx7ufz5/GHwBc9Z8y9VjZrB2275IlyUnEFYgmFk/M1thZgVm9lAZ65PNbGKwfo6ZtSix7uGgfYWZXV6ifa2ZLTazhWaWXx6DEZHoc31OU14Z1o0d+4q4aswMZq3SA3ei1UkDwczigdFAfyAbGGJm2aW6DQN2uHtr4HHgsWDbbCAXaAv0A8YE73fMJe7e0d1zzngkIhK1urVMY+rIXqSlhh64M3He+kiXJGUIZw+hK1Dg7qvdvQiYAAwq1WcQMD54PRnoa2YWtE9w90PuvgYoCN5PRKqY5mmpvHF3L3q0SuMXUxbz23eWcUS3u4gq4QRCE6Dk3asKg7Yy+7h7MbALSDvJtg58aGbzzWz48T7czIabWb6Z5W/dujWMckUkWtWunsifb+3CLT2a89y0NQx/KZ89B3WyOVqEEwhWRlvpWD9enxNt28vdOxE6FDXSzPqU9eHuPtbdc9w9Jz09PYxyRSSaJcTH8eigdvzHoLZ8+tVWrn16Juu/3R/psoTwAqEQaFpiOQPYeLw+ZpYA1Aa2n2hbdz/2cwvwJjqUJFKl3NyjBS/d3pXNuw8xaPR05qzWyeZICycQ5gFZZpZpZkmEThLnleqTBwwNXg8GPvHQ5Yl5QG4wCykTyALmmlmqmdUEMLNU4DJgyZkPR0Qqk16t6zN1ZC/qpiZx4/NzmDBXJ5sj6aSBEJwTuAf4AFgOTHL3pWb2qJkNDLqNA9LMrAB4EHgo2HYpMAlYBrwPjHT3I0BDYLqZLQLmAu+4+/vlOzQRqQwy66fyZnCy+aE3FvPoX5ZRrCubI8Iq031GcnJyPD9flyyIxKLiI0f5zTvLeXHmWvq0SWfUkAupXT0x0mVVemY2P9yp/bpSWUSiQkJ8HL8a2Jb/vKY9s1Zt4+rRM1i1dW+ky6pSFAgiElWGdG3Gq3d0Z+eBw1w1egafrtgS6ZKqDAWCiESdrpn1yLunF03qVOf2F+fx3GerdRvts0CBICJRKaNuClNG9OTyto347bvL+dnrX+iZzRVMgSAiUSs1OYHRP+rEA5dmMWVBIbljZ7N598FIlxWzFAgiEtXi4owHLm3D0zd24qvNe7hy1HQWrN8R6bJikgJBRCqF/u0b88bdPUlOjCP32dlMyt9w8o3klCgQRKTSOK9RLfJG9qZLZl1+PvkLfpW3VI/nLEcKBBGpVOqmJjH+tq4M653JizPXcsu4uWzfVxTpsmKCAkFEKp2E+DgeGZDNf1/Xgfnrd3DlqOks+XpXpMuq9BQIIlJpXds5g9fv6sFRd659eiZTP/860iVVagoEEanUOjStw1/u7U2HpnV4YOJC3RzvDCgQRKTSq18jmVfv6MZtvVrwwow13DRuDtv2Hop0WZWOAkFEYkJifBz/fmVb/uf6Dny+ficDR03ni8KdkS6rUlEgiEhMuaZTBlNG9MTMGPzMLF2vcAoUCCISc9o1qU3ePb3o0iJ0vcLDbyzmULHug3QyCgQRiUlpNZIZf1tXRnyvFa/NXc/1z85m484DkS4rqikQRCRmJcTH8Yt+5/HMTZ1ZtWUvA0ZNZ2bBtkiXFbUUCCIS8/q1a8Rb9/QiLTWJm8bN4Zm/r9LzFcoQViCYWT8zW2FmBWb2UBnrk81sYrB+jpm1KLHu4aB9hZldXmq7eDP73MzePtOBiIicSKv0Gkwd2Yv+7Rrz+/e+5K6X57P74OFIlxVVThoIZhYPjAb6A9nAEDPLLtVtGLDD3VsDjwOPBdtmA7lAW6AfMCZ4v2PuB5af6SBERMKRmpzAUz+6kP/3w/P5+MstXDlqOss27o50WVEjnD2ErkCBu6929yJgAjCoVJ9BwPjg9WSgr5lZ0D7B3Q+5+xqgIHg/zCwD+CHw/JkPQ0QkPGbGHRe1ZMLw7hw8fISrx8zQ1NRAOIHQBCj5v1Zh0FZmH3cvBnYBaSfZ9gng54CuMReRs65Li3q8fe9FdG4empr6i8l6RGc4gWBltJU+G3O8PmW2m9kAYIu7zz/ph5sNN7N8M8vfunXryasVEQlTes1kXh7WjZGXtGJi/gauGTOTdd/ui3RZERNOIBQCTUssZwAbj9fHzBKA2sD2E2zbCxhoZmsJHYL6vpm9UtaHu/tYd89x95z09PQwyhURCV/AGT/AAAAJDUlEQVR8nPEvl5/HuKE5FO7Yz4Anp/Pe4k2RLisiwgmEeUCWmWWaWRKhk8R5pfrkAUOD14OBTzw0pysPyA1mIWUCWcBcd3/Y3TPcvUXwfp+4+03lMB4RkdPS9/yGvHPfRbRsUIMRry7gV3lLq9zVzScNhOCcwD3AB4RmBE1y96Vm9qiZDQy6jQPSzKwAeBB4KNh2KTAJWAa8D4x096r1v7CIVBpN66Xw+l09uK1XC16cuZbrn5nFhu37I13WWWOV6eKMnJwcz8/Pj3QZIlIFvL/kG/5l8iIM+ON1HbisbaNIl3RazGy+u+eE01dXKouIlKFfu0a8c+9FNE9LZfjL8/mPt5dRVBzbkyIVCCIix9EsLYXJI3owtEdzxk1fw+BnYnsWkgJBROQEkhPi+fWgdjxzU2fWbtvHD5+czl8WlZ5oGRsUCCIiYejXrhHv3n8RbRrW4N7XPufhN77gQFFszZFRIIiIhCmjbgoT7+oRPGNhA4NGT+erzXsiXVa5USCIiJyCxOAZCy/d3pXt+4oY+NR0Xp2zLiZup61AEBE5DX3apPPu/RfRpUU9/u3NJYx4ZQE79xdFuqwzokAQETlNDWpWY/xtXfnXK87j4y830++Jacxa9W2kyzptCgQRkTMQF2cM79OKN0b0onpSPD96fjZ//GAFh49UvmsWFAgiIuWgfUZt3r63N4M7ZfDU3wq4/tlZle6aBQWCiEg5SU1O4L+u68CoIRdSsGUvV/xpGpPmbag0J5wVCCIi5ezKDufw/gN9aJ9Rm59P+YIRryxg+77oP+GsQBARqQBN6lTnf+/ozsP9j51w/oy/fxXdD/lSIIiIVJC4OOOui1sxdWQvaldPZOgLc/lV3tKofVSnAkFEpIK1Pac2f7m3N7f2DD1nYcCo6XxRuDPSZX2HAkFE5CyolhjPrwa25aXbu7L3YDFXj5nJ4x99FVXTUxUIIiJnUZ826XzwQB8GdjiHP328kmvGzGRllNwPSYEgInKW1U5J5PEbOvL0jZ0o3LGfH46azvPTVnP0aGSnpyoQREQipH/7xnz4k4vpk1Wf37yznNznZkf0YrawAsHM+pnZCjMrMLOHylifbGYTg/VzzKxFiXUPB+0rzOzyoK2amc01s0VmttTMfl1eAxIRqUzSaybz3C05/GHwBSzfuJt+T0zjxRlrIrK3cNJAMLN4YDTQH8gGhphZdqluw4Ad7t4aeBx4LNg2G8gF2gL9gDHB+x0Cvu/uHYCOQD8z614+QxIRqVzMjOtzmvLhg33omlmPX/1lWUT2FsLZQ+gKFLj7ancvAiYAg0r1GQSMD15PBvqamQXtE9z9kLuvAQqArh6yN+ifGPyrHNd2i4hUkMa1q/PibV34w7WR2VsIJxCaABtKLBcGbWX2cfdiYBeQdqJtzSzezBYCW4CP3H3O6QxARCSWmBnXd2nKBz/5572F/UXFFf7Z4QSCldFWOq6O1+e427r7EXfvCGQAXc2sXZkfbjbczPLNLH/r1ui+7FtEpLycU+cfewuZaamkJCVU+GeGEwiFQNMSyxnAxuP1MbMEoDawPZxt3X0n8Cmhcwzf4e5j3T3H3XPS09PDKFdEJDYc21t4bPAFZ+XzwgmEeUCWmWWaWRKhk8R5pfrkAUOD14OBTzx0v9c8IDeYhZQJZAFzzSzdzOoAmFl14FLgyzMfjoiInK6T7oO4e7GZ3QN8AMQDL7j7UjN7FMh39zxgHPCymRUQ2jPIDbZdamaTgGVAMTDS3Y+YWWNgfDDjKA6Y5O5vV8QARUQkPFZZHtwAkJOT4/n5+ZEuQ0Sk0jCz+e6eE05fXaksIiKAAkFERAIKBBERARQIIiISUCCIiAhQyWYZmdlWYN1pbl4f2FaO5VQWGnfVonFXLeGMu7m7h3VVb6UKhDNhZvnhTr2KJRp31aJxVy3lPW4dMhIREUCBICIigaoUCGMjXUCEaNxVi8ZdtZTruKvMOQQRETmxqrSHICIiJxDzgWBm/cxshZkVmNlDka6nIpnZC2a2xcyWlGirZ2YfmdnK4GfdSNZY3sysqZn9zcyWm9lSM7s/aI/pcQOYWTUzm2tmi4Kx/zpozzSzOcHYJwa3rY8pwRMXPzezt4PlmB8zgJmtNbPFZrbQzPKDtnL7XY/pQAhurz0a6A9kA0PMLDuyVVWoF/nug4YeAj529yzg42A5lhQDP3X384HuwMjg/+NYHzfAIeD77t4B6Aj0M7PuwGPA48HYdwDDIlhjRbkfWF5iuSqM+ZhL3L1jiemm5fa7HtOBAHQFCtx9tbsXAROAQRGuqcK4+2eEnkdR0iBgfPB6PHDVWS2qgrn7JndfELzeQ+iPRBNifNwAHrI3WEwM/jnwfWBy0B5zYzezDOCHwPPBshHjYz6Jcvtdj/VAaAJsKLFcGLRVJQ3dfROE/ngCDSJcT4UxsxbAhcAcqsi4g0MnC4EtwEfAKmCnux97Inss/s4/AfwcOBospxH7Yz7GgQ/NbL6ZDQ/ayu13veKf2hxZVkabplXFIDOrAUwBHnD33aEvjbHP3Y8AHYNH0r4JnF9Wt7NbVcUxswHAFnefb2bfO9ZcRteYGXMpvdx9o5k1AD4ys3J99HCs7yEUAk1LLGcAGyNUS6RsDh5ZSvBzS4TrKXdmlkgoDF519zeC5pgfd0nuvhP4lNB5lDpmduzLXqz9zvcCBprZWkKHgL9PaI8hlsf8f9x9Y/BzC6EvAF0px9/1WA+EeUBWMAMhidCznvMiXNPZlgcMDV4PBd6KYC3lLjh+PA5Y7u7/U2JVTI8bwMzSgz0DzKw6cCmhcyh/AwYH3WJq7O7+sLtnuHsLQv89f+LuNxLDYz7GzFLNrOax18BlwBLK8Xc95i9MM7MrCH2DiAdecPffRrikCmNmrwHfI3QHxM3AvwNTgUlAM2A9cJ27lz7xXGmZWW9gGrCYfxxT/ldC5xFidtwAZnYBoZOI8YS+3E1y90fNrCWhb8/1gM+Bm9z9UOQqrRjBIaOfufuAqjDmYIxvBosJwP+6+2/NLI1y+l2P+UAQEZHwxPohIxERCZMCQUREAAWCiIgEFAgiIgIoEEREJKBAEBERQIEgIiIBBYKIiADw/wFHnGmgPei1BQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot epochs and learning rate\n",
    "plt.plot(epoch_list, lr_list)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Set a reducing decay using \n",
    "decay = lr/Number of epochs\n",
    "\n",
    "\n",
    "# Set decay constant, lr and starting epoch \n",
    "decay = 0.001\n",
    "lr = 0.01\n",
    "epoch = 0\n",
    "# collect lr and epoch values\n",
    "lr_list = []\n",
    "epoch_list = []\n",
    "# setup while loop for learning rate update\n",
    "while epoch < 50:\n",
    "    lr = lr * (1/(1+decay*epoch))\n",
    "    epoch_list.append(epoch)\n",
    "    lr_list.append(lr)\n",
    "    epoch += 1    \n",
    "    # print(lr)\n",
    "# Verify the length of the lists.\n",
    "len(epoch_list), len(lr_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules useful to working\n",
    "# Fix random seed\n",
    "# Load dataset\n",
    "dataset = pd.read_csv('ionosphere.data')\n",
    "dataset = dataset.values\n",
    "# split into X and Y\n",
    "X = dataset[:,0:34].astype(float)\n",
    "Y = dataset[:,34]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode class values as integers\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(Y)\n",
    "encoded_Y = encoder.transform(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 234 samples, validate on 116 samples\n",
      "Epoch 1/50\n",
      " - 0s - loss: 0.6906 - accuracy: 0.6026 - val_loss: 0.6814 - val_accuracy: 0.6983\n",
      "Epoch 2/50\n",
      " - 0s - loss: 0.6742 - accuracy: 0.6795 - val_loss: 0.6072 - val_accuracy: 0.8621\n",
      "Epoch 3/50\n",
      " - 0s - loss: 0.6245 - accuracy: 0.7735 - val_loss: 0.4987 - val_accuracy: 0.8879\n",
      "Epoch 4/50\n",
      " - 0s - loss: 0.5472 - accuracy: 0.8162 - val_loss: 0.4475 - val_accuracy: 0.9569\n",
      "Epoch 5/50\n",
      " - 0s - loss: 0.4528 - accuracy: 0.8291 - val_loss: 0.3826 - val_accuracy: 0.9224\n",
      "Epoch 6/50\n",
      " - 0s - loss: 0.3764 - accuracy: 0.8632 - val_loss: 0.3780 - val_accuracy: 0.8879\n",
      "Epoch 7/50\n",
      " - 0s - loss: 0.3312 - accuracy: 0.8718 - val_loss: 0.2983 - val_accuracy: 0.9224\n",
      "Epoch 8/50\n",
      " - 0s - loss: 0.2838 - accuracy: 0.8932 - val_loss: 0.2615 - val_accuracy: 0.9224\n",
      "Epoch 9/50\n",
      " - 0s - loss: 0.2490 - accuracy: 0.9017 - val_loss: 0.2027 - val_accuracy: 0.9397\n",
      "Epoch 10/50\n",
      " - 0s - loss: 0.2259 - accuracy: 0.9188 - val_loss: 0.1915 - val_accuracy: 0.9569\n",
      "Epoch 11/50\n",
      " - 0s - loss: 0.2116 - accuracy: 0.9145 - val_loss: 0.2080 - val_accuracy: 0.9310\n",
      "Epoch 12/50\n",
      " - 0s - loss: 0.2143 - accuracy: 0.9188 - val_loss: 0.1519 - val_accuracy: 0.9569\n",
      "Epoch 13/50\n",
      " - 0s - loss: 0.1831 - accuracy: 0.9274 - val_loss: 0.1727 - val_accuracy: 0.9569\n",
      "Epoch 14/50\n",
      " - 0s - loss: 0.1650 - accuracy: 0.9402 - val_loss: 0.2557 - val_accuracy: 0.9052\n",
      "Epoch 15/50\n",
      " - 0s - loss: 0.1999 - accuracy: 0.9188 - val_loss: 0.1385 - val_accuracy: 0.9655\n",
      "Epoch 16/50\n",
      " - 0s - loss: 0.1545 - accuracy: 0.9444 - val_loss: 0.1468 - val_accuracy: 0.9569\n",
      "Epoch 17/50\n",
      " - 0s - loss: 0.1457 - accuracy: 0.9487 - val_loss: 0.1059 - val_accuracy: 0.9741\n",
      "Epoch 18/50\n",
      " - 0s - loss: 0.1359 - accuracy: 0.9573 - val_loss: 0.1902 - val_accuracy: 0.9483\n",
      "Epoch 19/50\n",
      " - 0s - loss: 0.1386 - accuracy: 0.9487 - val_loss: 0.0877 - val_accuracy: 0.9741\n",
      "Epoch 20/50\n",
      " - 0s - loss: 0.1304 - accuracy: 0.9573 - val_loss: 0.1612 - val_accuracy: 0.9483\n",
      "Epoch 21/50\n",
      " - 0s - loss: 0.1206 - accuracy: 0.9487 - val_loss: 0.0837 - val_accuracy: 0.9741\n",
      "Epoch 22/50\n",
      " - 0s - loss: 0.1300 - accuracy: 0.9573 - val_loss: 0.1846 - val_accuracy: 0.9483\n",
      "Epoch 23/50\n",
      " - 0s - loss: 0.1274 - accuracy: 0.9573 - val_loss: 0.1078 - val_accuracy: 0.9828\n",
      "Epoch 24/50\n",
      " - 0s - loss: 0.1179 - accuracy: 0.9658 - val_loss: 0.0945 - val_accuracy: 0.9741\n",
      "Epoch 25/50\n",
      " - 0s - loss: 0.1089 - accuracy: 0.9615 - val_loss: 0.1069 - val_accuracy: 0.9741\n",
      "Epoch 26/50\n",
      " - 0s - loss: 0.1052 - accuracy: 0.9658 - val_loss: 0.0901 - val_accuracy: 0.9828\n",
      "Epoch 27/50\n",
      " - 0s - loss: 0.1032 - accuracy: 0.9701 - val_loss: 0.1054 - val_accuracy: 0.9741\n",
      "Epoch 28/50\n",
      " - 0s - loss: 0.1003 - accuracy: 0.9658 - val_loss: 0.0840 - val_accuracy: 0.9828\n",
      "Epoch 29/50\n",
      " - 0s - loss: 0.0993 - accuracy: 0.9744 - val_loss: 0.1076 - val_accuracy: 0.9741\n",
      "Epoch 30/50\n",
      " - 0s - loss: 0.0942 - accuracy: 0.9701 - val_loss: 0.1214 - val_accuracy: 0.9741\n",
      "Epoch 31/50\n",
      " - 0s - loss: 0.0913 - accuracy: 0.9701 - val_loss: 0.0861 - val_accuracy: 0.9828\n",
      "Epoch 32/50\n",
      " - 0s - loss: 0.0890 - accuracy: 0.9744 - val_loss: 0.0932 - val_accuracy: 0.9828\n",
      "Epoch 33/50\n",
      " - 0s - loss: 0.0851 - accuracy: 0.9744 - val_loss: 0.0878 - val_accuracy: 0.9828\n",
      "Epoch 34/50\n",
      " - 0s - loss: 0.0873 - accuracy: 0.9786 - val_loss: 0.1308 - val_accuracy: 0.9655\n",
      "Epoch 35/50\n",
      " - 0s - loss: 0.0885 - accuracy: 0.9786 - val_loss: 0.0963 - val_accuracy: 0.9828\n",
      "Epoch 36/50\n",
      " - 0s - loss: 0.0824 - accuracy: 0.9744 - val_loss: 0.0864 - val_accuracy: 0.9828\n",
      "Epoch 37/50\n",
      " - 0s - loss: 0.0782 - accuracy: 0.9744 - val_loss: 0.0857 - val_accuracy: 0.9828\n",
      "Epoch 38/50\n",
      " - 0s - loss: 0.0766 - accuracy: 0.9744 - val_loss: 0.0749 - val_accuracy: 0.9828\n",
      "Epoch 39/50\n",
      " - 0s - loss: 0.0786 - accuracy: 0.9786 - val_loss: 0.0904 - val_accuracy: 0.9828\n",
      "Epoch 40/50\n",
      " - 0s - loss: 0.0753 - accuracy: 0.9829 - val_loss: 0.0829 - val_accuracy: 0.9828\n",
      "Epoch 41/50\n",
      " - 0s - loss: 0.0721 - accuracy: 0.9829 - val_loss: 0.0873 - val_accuracy: 0.9828\n",
      "Epoch 42/50\n",
      " - 0s - loss: 0.0703 - accuracy: 0.9829 - val_loss: 0.0845 - val_accuracy: 0.9828\n",
      "Epoch 43/50\n",
      " - 0s - loss: 0.0701 - accuracy: 0.9872 - val_loss: 0.0700 - val_accuracy: 0.9828\n",
      "Epoch 44/50\n",
      " - 0s - loss: 0.0748 - accuracy: 0.9786 - val_loss: 0.0917 - val_accuracy: 0.9828\n",
      "Epoch 45/50\n",
      " - 0s - loss: 0.0665 - accuracy: 0.9872 - val_loss: 0.0790 - val_accuracy: 0.9828\n",
      "Epoch 46/50\n",
      " - 0s - loss: 0.0646 - accuracy: 0.9829 - val_loss: 0.0779 - val_accuracy: 0.9828\n",
      "Epoch 47/50\n",
      " - 0s - loss: 0.0654 - accuracy: 0.9872 - val_loss: 0.0837 - val_accuracy: 0.9828\n",
      "Epoch 48/50\n",
      " - 0s - loss: 0.0659 - accuracy: 0.9829 - val_loss: 0.0887 - val_accuracy: 0.9828\n",
      "Epoch 49/50\n",
      " - 0s - loss: 0.0653 - accuracy: 0.9786 - val_loss: 0.0717 - val_accuracy: 0.9828\n",
      "Epoch 50/50\n",
      " - 0s - loss: 0.0662 - accuracy: 0.9786 - val_loss: 0.0883 - val_accuracy: 0.9741\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7fa2c02c79b0>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create model\n",
    "model = Sequential()\n",
    "model.add(Dense(34, input_dim=34, kernel_initializer='uniform', activation='relu'))\n",
    "model.add(Dense(1, kernel_initializer='uniform', activation='sigmoid'))\n",
    "# compile model\n",
    "epochs = 50\n",
    "learning_rate = 0.1\n",
    "decay_rate = learning_rate / epochs\n",
    "momentum = 0.8\n",
    "sgd = SGD(learning_rate=learning_rate, momentum=momentum, decay=decay_rate, nesterov=False)\n",
    "model.compile(loss = 'binary_crossentropy', optimizer= sgd, metrics = ['accuracy'])\n",
    "# Fit the model\n",
    "model.fit(X, encoded_Y, validation_split=0.33, epochs = epochs, batch_size=28, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT RUN\n",
    "# Set decay constant, lr and starting epoch \n",
    "decay = 0.001\n",
    "lr = 0.01\n",
    "epoch = 0\n",
    "# collect lr and epoch values\n",
    "lr_list = []\n",
    "epoch_list = []\n",
    "# setup while loop for learning rate update\n",
    "\n",
    "while epoch < 50:\n",
    "    if epoch%10 == 0:\n",
    "        lr = lr * (1/(1+decay*epoch))\n",
    "        epoch_list.append(epoch)\n",
    "        lr_list.append(lr)\n",
    "    else:\n",
    "        epoch += 1    \n",
    "    # print(lr)\n",
    "# Verify the length of the lists.\n",
    "len(epoch_list), len(lr_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LearningRate = InitialLearningRate * DropRate ^ (floor((1+epoch)/epochDrop))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning rate schedule using the learning rate scheduler\n",
    "import math\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "# define learning rate schedule\n",
    "def step_decay(epoch):\n",
    "    initial_lrate = 0.1\n",
    "    drop = 0.5\n",
    "    epochs_drop = 100.0\n",
    "    lrate = initial_lrate * math.pow(drop, math.floor((1 + epoch)/epochs_drop))\n",
    "    return lrate\n",
    "\n",
    "# Set random seed\n",
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "# Load dataset\n",
    "# Split dataset\n",
    "# Encode Y variable\n",
    "# Create model\n",
    "model = Sequential()\n",
    "model.add(Dense(34, input_dim=34, kernel_initializer='uniform', activation = 'relu'))\n",
    "model.add(Dense(1, kernel_initializer='normal', activation = 'sigmoid'))\n",
    "\n",
    "# compile the model\n",
    "sgd = SGD(lr=0.0, momentum=0.9, decay=0.0, nesterov = False)\n",
    "model.compile(loss= 'binary_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "\n",
    "# learning schedule callback\n",
    "lrate = LearningRateScheduler(step_decay)\n",
    "callbacks_list = [lrate]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 234 samples, validate on 116 samples\n",
      "Epoch 1/50\n",
      " - 0s - loss: 0.6883 - accuracy: 0.6282 - val_loss: 0.6523 - val_accuracy: 0.7328\n",
      "Epoch 2/50\n",
      " - 0s - loss: 0.6500 - accuracy: 0.6111 - val_loss: 0.4649 - val_accuracy: 0.9310\n",
      "Epoch 3/50\n",
      " - 0s - loss: 0.5678 - accuracy: 0.7650 - val_loss: 0.5561 - val_accuracy: 0.7155\n",
      "Epoch 4/50\n",
      " - 0s - loss: 0.4541 - accuracy: 0.7821 - val_loss: 0.1963 - val_accuracy: 0.9569\n",
      "Epoch 5/50\n",
      " - 0s - loss: 0.3832 - accuracy: 0.8504 - val_loss: 0.3187 - val_accuracy: 0.9310\n",
      "Epoch 6/50\n",
      " - 0s - loss: 0.2853 - accuracy: 0.8932 - val_loss: 0.2564 - val_accuracy: 0.9224\n",
      "Epoch 7/50\n",
      " - 0s - loss: 0.2462 - accuracy: 0.8889 - val_loss: 0.2705 - val_accuracy: 0.9052\n",
      "Epoch 8/50\n",
      " - 0s - loss: 0.2234 - accuracy: 0.9274 - val_loss: 0.1568 - val_accuracy: 0.9569\n",
      "Epoch 9/50\n",
      " - 0s - loss: 0.1753 - accuracy: 0.9316 - val_loss: 0.1612 - val_accuracy: 0.9397\n",
      "Epoch 10/50\n",
      " - 0s - loss: 0.1642 - accuracy: 0.9402 - val_loss: 0.2050 - val_accuracy: 0.9569\n",
      "Epoch 11/50\n",
      " - 0s - loss: 0.1496 - accuracy: 0.9487 - val_loss: 0.1242 - val_accuracy: 0.9655\n",
      "Epoch 12/50\n",
      " - 0s - loss: 0.1597 - accuracy: 0.9359 - val_loss: 0.0804 - val_accuracy: 0.9741\n",
      "Epoch 13/50\n",
      " - 0s - loss: 0.1394 - accuracy: 0.9487 - val_loss: 0.2328 - val_accuracy: 0.9483\n",
      "Epoch 14/50\n",
      " - 0s - loss: 0.1370 - accuracy: 0.9530 - val_loss: 0.0897 - val_accuracy: 0.9828\n",
      "Epoch 15/50\n",
      " - 0s - loss: 0.1072 - accuracy: 0.9658 - val_loss: 0.0933 - val_accuracy: 0.9741\n",
      "Epoch 16/50\n",
      " - 0s - loss: 0.0940 - accuracy: 0.9658 - val_loss: 0.0794 - val_accuracy: 0.9828\n",
      "Epoch 17/50\n",
      " - 0s - loss: 0.0916 - accuracy: 0.9658 - val_loss: 0.0703 - val_accuracy: 0.9828\n",
      "Epoch 18/50\n",
      " - 0s - loss: 0.0824 - accuracy: 0.9786 - val_loss: 0.0862 - val_accuracy: 0.9828\n",
      "Epoch 19/50\n",
      " - 0s - loss: 0.0770 - accuracy: 0.9786 - val_loss: 0.0741 - val_accuracy: 0.9914\n",
      "Epoch 20/50\n",
      " - 0s - loss: 0.0681 - accuracy: 0.9786 - val_loss: 0.0906 - val_accuracy: 0.9914\n",
      "Epoch 21/50\n",
      " - 0s - loss: 0.0659 - accuracy: 0.9829 - val_loss: 0.0571 - val_accuracy: 0.9828\n",
      "Epoch 22/50\n",
      " - 0s - loss: 0.0628 - accuracy: 0.9829 - val_loss: 0.0938 - val_accuracy: 0.9914\n",
      "Epoch 23/50\n",
      " - 0s - loss: 0.0572 - accuracy: 0.9872 - val_loss: 0.0580 - val_accuracy: 0.9914\n",
      "Epoch 24/50\n",
      " - 0s - loss: 0.0562 - accuracy: 0.9829 - val_loss: 0.0889 - val_accuracy: 0.9914\n",
      "Epoch 25/50\n",
      " - 0s - loss: 0.0548 - accuracy: 0.9872 - val_loss: 0.0523 - val_accuracy: 0.9914\n",
      "Epoch 26/50\n",
      " - 0s - loss: 0.0535 - accuracy: 0.9872 - val_loss: 0.0576 - val_accuracy: 0.9914\n",
      "Epoch 27/50\n",
      " - 0s - loss: 0.0475 - accuracy: 0.9872 - val_loss: 0.0607 - val_accuracy: 0.9914\n",
      "Epoch 28/50\n",
      " - 0s - loss: 0.0440 - accuracy: 0.9872 - val_loss: 0.0689 - val_accuracy: 0.9914\n",
      "Epoch 29/50\n",
      " - 0s - loss: 0.0432 - accuracy: 0.9872 - val_loss: 0.0630 - val_accuracy: 0.9914\n",
      "Epoch 30/50\n",
      " - 0s - loss: 0.0406 - accuracy: 0.9915 - val_loss: 0.0625 - val_accuracy: 0.9914\n",
      "Epoch 31/50\n",
      " - 0s - loss: 0.0408 - accuracy: 0.9915 - val_loss: 0.0771 - val_accuracy: 0.9741\n",
      "Epoch 32/50\n",
      " - 0s - loss: 0.0506 - accuracy: 0.9915 - val_loss: 0.0514 - val_accuracy: 0.9828\n",
      "Epoch 33/50\n",
      " - 0s - loss: 0.0425 - accuracy: 0.9872 - val_loss: 0.1021 - val_accuracy: 0.9828\n",
      "Epoch 34/50\n",
      " - 0s - loss: 0.0458 - accuracy: 0.9915 - val_loss: 0.0538 - val_accuracy: 0.9828\n",
      "Epoch 35/50\n",
      " - 0s - loss: 0.0447 - accuracy: 0.9872 - val_loss: 0.0737 - val_accuracy: 0.9914\n",
      "Epoch 36/50\n",
      " - 0s - loss: 0.0358 - accuracy: 0.9957 - val_loss: 0.0542 - val_accuracy: 0.9828\n",
      "Epoch 37/50\n",
      " - 0s - loss: 0.0343 - accuracy: 0.9957 - val_loss: 0.0738 - val_accuracy: 0.9914\n",
      "Epoch 38/50\n",
      " - 0s - loss: 0.0328 - accuracy: 0.9957 - val_loss: 0.0563 - val_accuracy: 0.9828\n",
      "Epoch 39/50\n",
      " - 0s - loss: 0.0312 - accuracy: 0.9915 - val_loss: 0.0755 - val_accuracy: 0.9828\n",
      "Epoch 40/50\n",
      " - 0s - loss: 0.0295 - accuracy: 0.9957 - val_loss: 0.0590 - val_accuracy: 0.9914\n",
      "Epoch 41/50\n",
      " - 0s - loss: 0.0315 - accuracy: 0.9957 - val_loss: 0.0681 - val_accuracy: 0.9914\n",
      "Epoch 42/50\n",
      " - 0s - loss: 0.0282 - accuracy: 0.9957 - val_loss: 0.0628 - val_accuracy: 0.9914\n",
      "Epoch 43/50\n",
      " - 0s - loss: 0.0264 - accuracy: 0.9957 - val_loss: 0.0594 - val_accuracy: 0.9914\n",
      "Epoch 44/50\n",
      " - 0s - loss: 0.0292 - accuracy: 0.9957 - val_loss: 0.0713 - val_accuracy: 0.9828\n",
      "Epoch 45/50\n",
      " - 0s - loss: 0.0268 - accuracy: 0.9957 - val_loss: 0.0595 - val_accuracy: 0.9828\n",
      "Epoch 46/50\n",
      " - 0s - loss: 0.0285 - accuracy: 0.9957 - val_loss: 0.0681 - val_accuracy: 0.9914\n",
      "Epoch 47/50\n",
      " - 0s - loss: 0.0249 - accuracy: 0.9957 - val_loss: 0.0647 - val_accuracy: 0.9828\n",
      "Epoch 48/50\n",
      " - 0s - loss: 0.0241 - accuracy: 0.9957 - val_loss: 0.0690 - val_accuracy: 0.9828\n",
      "Epoch 49/50\n",
      " - 0s - loss: 0.0268 - accuracy: 0.9957 - val_loss: 0.0603 - val_accuracy: 0.9828\n",
      "Epoch 50/50\n",
      " - 0s - loss: 0.0257 - accuracy: 0.9957 - val_loss: 0.0659 - val_accuracy: 0.9828\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7f628c7359e8>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the model\n",
    "model.fit(X, encoded_Y, validation_split=0.33, epochs=50, batch_size=28, callbacks=callbacks_list, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exponential Learning rate decay\n",
    "def exp_decay(epoch):\n",
    "    initial_lrate = 0.1\n",
    "    k = 0.1\n",
    "    lrate = initial_lrate * exp(-k*t)\n",
    "    return lrate\n",
    "\n",
    "# Implement with LearningRateScheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning Rate Schedule Tips:\n",
    "    \n",
    "    1. Increase the initial learning rate. Because the learning rate will decrease, start with a larger value to decrease from. A larger learning rate will result in a lot larger changes to the weights, at least in the beginning, allowing you to benefit from fine tuning later.\n",
    "    2. Use a large momentum. Using a larger momentum value will help the optimization algorithm to continue to make updates in the right direction when your learning rate shrinks to small values.\n",
    "    3. Experiment with different schedules. It will not be clear which learning rate schedule to use so try a few with di↵erent configuration options and see what works best on your problem. Also try schedules that change exponentially and even schedules that respond to the accuracy of your model on the training or test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
